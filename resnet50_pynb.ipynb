{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "resnet50.pynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mtrefilek/cs762/blob/main/resnet50_pynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "WXyqou2bMLFc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "cwd = os.getcwd().replace('\\\\','/')\n",
        "\n",
        "import numpy as np\n",
        "import pickle\n",
        "from copy import deepcopy\n",
        "from torchvision import datasets\n",
        "from glob import glob, iglob\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "from torch.autograd import Variable"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DSET_NAME = 'FMNIST' #('MNIST', 'FMNIST', 'CIFAR10', 'CIFAR100', 'PlantDisease', 'EuroSAT', 'ChestXRay') 'ISIC2018', 'TinyImageNet' are not yet implemented\n",
        "PRETRAINED_MODEL_NAME = 'RESNET50'"
      ],
      "metadata": {
        "id": "k8rnI0dmOlP6"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Z-kuh9Y5MmZ",
        "outputId": "d394ccff-3ff9-49d4-bbd8-d6684f91eca9"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pretrained model\n",
        "model = models.resnet50(pretrained=True)\n",
        "# Use the model object to select the desired layer\n",
        "layer = model._modules.get('avgpool')\n",
        "feature_extractor = torch.nn.Sequential(*list(model.children())[:-1])\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n"
      ],
      "metadata": {
        "id": "orw58vQOHnGt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5ce31d7-0cdc-47ba-ea74-0bb35b139d50"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (4): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (5): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set model to evaluation mode\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "9BrCWD-cHqRe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08d37ba1-d438-4390-cef1-0794ffec86e2"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (4): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (5): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def batch(iterable, n=1):\n",
        "    l = len(iterable)\n",
        "    for ndx in range(0, l, n):\n",
        "        yield iterable[ndx:min(ndx + n, l)]\n",
        "\n",
        "#import torchvision\n",
        "transform=transforms.Compose([\n",
        "    # you can add other transformations in this list\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "dset_path = cwd + '/dataset'\n",
        "feature_path = cwd+'/extracted_features/'\n",
        "\n",
        "if DSET_NAME=='MNIST': # MNIST\n",
        "    mnist_train = datasets.MNIST(root=dset_path, train=True, download=True)#, transform=transform)\n",
        "    mnist_test = datasets.MNIST(root=dset_path, train=False, download=True)#, transform=transform)\n",
        "    imgs_tr, imgs_tst, labels_tr, labels_tst = [], [], [], []\n",
        "    for (img, label) in mnist_train:\n",
        "        img = Variable(transform(img.convert(mode='RGB')))\n",
        "        imgs_tr.append(img)\n",
        "        labels_tr.append(label)\n",
        "    for (img, label) in mnist_test:\n",
        "        img = Variable(transform(img.convert(mode='RGB')))\n",
        "        imgs_tst.append(img)\n",
        "        labels_tst.append(label)\n",
        "    classnames = [str(a) for a in range(10)]\n",
        "    \n",
        "elif DSET_NAME=='FMNIST': # Fashion-MNIST\n",
        "    trainset = datasets.FashionMNIST(root=dset_path, train=True, download=True)\n",
        "    testset = datasets.FashionMNIST(root=dset_path, train=False, download=True)\n",
        "    imgs_tr, imgs_tst, labels_tr, labels_tst = [], [], [], []\n",
        "    for (img, label) in trainset:\n",
        "        img = Variable(transform(img.convert(mode='RGB')))\n",
        "        imgs_tr.append(img)\n",
        "        labels_tr.append(label)\n",
        "    for (img, label) in testset:\n",
        "        img = Variable(transform(img.convert(mode='RGB')))\n",
        "        imgs_tst.append(img)\n",
        "        labels_tst.append(label)\n",
        "    classnames = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "\n",
        "elif DSET_NAME=='CIFAR10': # CIFAR-10\n",
        "    trainset = datasets.CIFAR10(root=dset_path, train=True, download=True)\n",
        "    testset = datasets.CIFAR10(root=dset_path, train=False, download=True)\n",
        "    imgs_tr, imgs_tst, labels_tr, labels_tst = [], [], [], []\n",
        "    for (img, label) in trainset:\n",
        "        img = Variable(transform(img.convert(mode='RGB')))\n",
        "        imgs_tr.append(img)\n",
        "        labels_tr.append(label)\n",
        "    for (img, label) in testset:\n",
        "        img = Variable(transform(img.convert(mode='RGB')))\n",
        "        imgs_tst.append(img)\n",
        "        labels_tst.append(label)\n",
        "    meta = pickle.load( open(dset_path+'/cifar-10-batches-py/batches.meta','rb') )\n",
        "    classnames = meta['label_names']\n",
        "\n",
        "elif DSET_NAME=='CIFAR100': # CIFAR-100\n",
        "    trainset = datasets.CIFAR100(root=dset_path, train=True, download=True)\n",
        "    testset = datasets.CIFAR100(root=dset_path, train=False, download=True)\n",
        "    imgs_tr, imgs_tst, labels_tr, labels_tst = [], [], [], []\n",
        "    for (img, label) in trainset:\n",
        "        img = Variable(transform(img.convert(mode='RGB')))\n",
        "        imgs_tr.append(img)\n",
        "        labels_tr.append(label)\n",
        "    for (img, label) in testset:\n",
        "        img = Variable(transform(img.convert(mode='RGB')))\n",
        "        imgs_tst.append(img)\n",
        "        labels_tst.append(label)\n",
        "    meta = pickle.load( open(dset_path+'/cifar-100-python/meta','rb') )\n",
        "    classnames = meta['fine_label_names']"
      ],
      "metadata": {
        "id": "qm8lBW0pPapo"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Extract Features\n",
        "if DSET_NAME in ('MNIST', 'FMNIST', 'CIFAR10', 'CIFAR100'):\n",
        "    n_tr = len(imgs_tr)\n",
        "    feature_matrix_tr = np.zeros((1, 2048))\n",
        "    for img_batch in batch(imgs_tr, n=256):\n",
        "        inputs = torch.stack(img_batch).to(device)\n",
        "        img_features_tr = feature_extractor(inputs).squeeze().cpu().detach().numpy()\n",
        "        feature_matrix_tr = np.concatenate((feature_matrix_tr, img_features_tr), axis=0)\n",
        "        print('Extracting Training Features: {0:.2f}% done'.format(100*len(feature_matrix_tr[1:])/n_tr) )\n",
        "    feature_matrix_tr = feature_matrix_tr[1:]\n",
        "\n",
        "    n_cls = np.max(labels_tr)+1\n",
        "    labels_tr = np.array(labels_tr)\n",
        "    feature_matrices_tr = []\n",
        "    for i in range(n_cls):\n",
        "        feature_matrices_tr.append(feature_matrix_tr[labels_tr==i])\n",
        "\n",
        "    n_tst = len(imgs_tst)\n",
        "    feature_matrix_tst = np.zeros((1, 2048))\n",
        "    for img_batch in batch(imgs_tst, n=256):\n",
        "      inputs = torch.stack(img_batch).to(device)\n",
        "      img_features_tst = feature_extractor(inputs).squeeze().cpu().detach().numpy()\n",
        "      feature_matrix_tst = np.concatenate((feature_matrix_tst, img_features_tst), axis=0)\n",
        "      print('Extracting Test Features: {0:.2f}% done'.format(100*len(feature_matrix_tst[1:])/n_tst) )\n",
        "    feature_matrix_tst = feature_matrix_tst[1:]\n",
        "\n",
        "    n_cls = np.max(labels_tst)+1\n",
        "    labels_tst = np.array(labels_tst)\n",
        "    feature_matrices_tst = []\n",
        "    for i in range(n_cls):\n",
        "        feature_matrices_tst.append(feature_matrix_tst[labels_tst==i])\n",
        "    \n"
      ],
      "metadata": {
        "id": "6H9z2ApKPxQO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a36746d0-db57-46f0-9757-2d7d18f24730"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting Training Features: 0.43% done\n",
            "Extracting Training Features: 0.85% done\n",
            "Extracting Training Features: 1.28% done\n",
            "Extracting Training Features: 1.71% done\n",
            "Extracting Training Features: 2.13% done\n",
            "Extracting Training Features: 2.56% done\n",
            "Extracting Training Features: 2.99% done\n",
            "Extracting Training Features: 3.41% done\n",
            "Extracting Training Features: 3.84% done\n",
            "Extracting Training Features: 4.27% done\n",
            "Extracting Training Features: 4.69% done\n",
            "Extracting Training Features: 5.12% done\n",
            "Extracting Training Features: 5.55% done\n",
            "Extracting Training Features: 5.97% done\n",
            "Extracting Training Features: 6.40% done\n",
            "Extracting Training Features: 6.83% done\n",
            "Extracting Training Features: 7.25% done\n",
            "Extracting Training Features: 7.68% done\n",
            "Extracting Training Features: 8.11% done\n",
            "Extracting Training Features: 8.53% done\n",
            "Extracting Training Features: 8.96% done\n",
            "Extracting Training Features: 9.39% done\n",
            "Extracting Training Features: 9.81% done\n",
            "Extracting Training Features: 10.24% done\n",
            "Extracting Training Features: 10.67% done\n",
            "Extracting Training Features: 11.09% done\n",
            "Extracting Training Features: 11.52% done\n",
            "Extracting Training Features: 11.95% done\n",
            "Extracting Training Features: 12.37% done\n",
            "Extracting Training Features: 12.80% done\n",
            "Extracting Training Features: 13.23% done\n",
            "Extracting Training Features: 13.65% done\n",
            "Extracting Training Features: 14.08% done\n",
            "Extracting Training Features: 14.51% done\n",
            "Extracting Training Features: 14.93% done\n",
            "Extracting Training Features: 15.36% done\n",
            "Extracting Training Features: 15.79% done\n",
            "Extracting Training Features: 16.21% done\n",
            "Extracting Training Features: 16.64% done\n",
            "Extracting Training Features: 17.07% done\n",
            "Extracting Training Features: 17.49% done\n",
            "Extracting Training Features: 17.92% done\n",
            "Extracting Training Features: 18.35% done\n",
            "Extracting Training Features: 18.77% done\n",
            "Extracting Training Features: 19.20% done\n",
            "Extracting Training Features: 19.63% done\n",
            "Extracting Training Features: 20.05% done\n",
            "Extracting Training Features: 20.48% done\n",
            "Extracting Training Features: 20.91% done\n",
            "Extracting Training Features: 21.33% done\n",
            "Extracting Training Features: 21.76% done\n",
            "Extracting Training Features: 22.19% done\n",
            "Extracting Training Features: 22.61% done\n",
            "Extracting Training Features: 23.04% done\n",
            "Extracting Training Features: 23.47% done\n",
            "Extracting Training Features: 23.89% done\n",
            "Extracting Training Features: 24.32% done\n",
            "Extracting Training Features: 24.75% done\n",
            "Extracting Training Features: 25.17% done\n",
            "Extracting Training Features: 25.60% done\n",
            "Extracting Training Features: 26.03% done\n",
            "Extracting Training Features: 26.45% done\n",
            "Extracting Training Features: 26.88% done\n",
            "Extracting Training Features: 27.31% done\n",
            "Extracting Training Features: 27.73% done\n",
            "Extracting Training Features: 28.16% done\n",
            "Extracting Training Features: 28.59% done\n",
            "Extracting Training Features: 29.01% done\n",
            "Extracting Training Features: 29.44% done\n",
            "Extracting Training Features: 29.87% done\n",
            "Extracting Training Features: 30.29% done\n",
            "Extracting Training Features: 30.72% done\n",
            "Extracting Training Features: 31.15% done\n",
            "Extracting Training Features: 31.57% done\n",
            "Extracting Training Features: 32.00% done\n",
            "Extracting Training Features: 32.43% done\n",
            "Extracting Training Features: 32.85% done\n",
            "Extracting Training Features: 33.28% done\n",
            "Extracting Training Features: 33.71% done\n",
            "Extracting Training Features: 34.13% done\n",
            "Extracting Training Features: 34.56% done\n",
            "Extracting Training Features: 34.99% done\n",
            "Extracting Training Features: 35.41% done\n",
            "Extracting Training Features: 35.84% done\n",
            "Extracting Training Features: 36.27% done\n",
            "Extracting Training Features: 36.69% done\n",
            "Extracting Training Features: 37.12% done\n",
            "Extracting Training Features: 37.55% done\n",
            "Extracting Training Features: 37.97% done\n",
            "Extracting Training Features: 38.40% done\n",
            "Extracting Training Features: 38.83% done\n",
            "Extracting Training Features: 39.25% done\n",
            "Extracting Training Features: 39.68% done\n",
            "Extracting Training Features: 40.11% done\n",
            "Extracting Training Features: 40.53% done\n",
            "Extracting Training Features: 40.96% done\n",
            "Extracting Training Features: 41.39% done\n",
            "Extracting Training Features: 41.81% done\n",
            "Extracting Training Features: 42.24% done\n",
            "Extracting Training Features: 42.67% done\n",
            "Extracting Training Features: 43.09% done\n",
            "Extracting Training Features: 43.52% done\n",
            "Extracting Training Features: 43.95% done\n",
            "Extracting Training Features: 44.37% done\n",
            "Extracting Training Features: 44.80% done\n",
            "Extracting Training Features: 45.23% done\n",
            "Extracting Training Features: 45.65% done\n",
            "Extracting Training Features: 46.08% done\n",
            "Extracting Training Features: 46.51% done\n",
            "Extracting Training Features: 46.93% done\n",
            "Extracting Training Features: 47.36% done\n",
            "Extracting Training Features: 47.79% done\n",
            "Extracting Training Features: 48.21% done\n",
            "Extracting Training Features: 48.64% done\n",
            "Extracting Training Features: 49.07% done\n",
            "Extracting Training Features: 49.49% done\n",
            "Extracting Training Features: 49.92% done\n",
            "Extracting Training Features: 50.35% done\n",
            "Extracting Training Features: 50.77% done\n",
            "Extracting Training Features: 51.20% done\n",
            "Extracting Training Features: 51.63% done\n",
            "Extracting Training Features: 52.05% done\n",
            "Extracting Training Features: 52.48% done\n",
            "Extracting Training Features: 52.91% done\n",
            "Extracting Training Features: 53.33% done\n",
            "Extracting Training Features: 53.76% done\n",
            "Extracting Training Features: 54.19% done\n",
            "Extracting Training Features: 54.61% done\n",
            "Extracting Training Features: 55.04% done\n",
            "Extracting Training Features: 55.47% done\n",
            "Extracting Training Features: 55.89% done\n",
            "Extracting Training Features: 56.32% done\n",
            "Extracting Training Features: 56.75% done\n",
            "Extracting Training Features: 57.17% done\n",
            "Extracting Training Features: 57.60% done\n",
            "Extracting Training Features: 58.03% done\n",
            "Extracting Training Features: 58.45% done\n",
            "Extracting Training Features: 58.88% done\n",
            "Extracting Training Features: 59.31% done\n",
            "Extracting Training Features: 59.73% done\n",
            "Extracting Training Features: 60.16% done\n",
            "Extracting Training Features: 60.59% done\n",
            "Extracting Training Features: 61.01% done\n",
            "Extracting Training Features: 61.44% done\n",
            "Extracting Training Features: 61.87% done\n",
            "Extracting Training Features: 62.29% done\n",
            "Extracting Training Features: 62.72% done\n",
            "Extracting Training Features: 63.15% done\n",
            "Extracting Training Features: 63.57% done\n",
            "Extracting Training Features: 64.00% done\n",
            "Extracting Training Features: 64.43% done\n",
            "Extracting Training Features: 64.85% done\n",
            "Extracting Training Features: 65.28% done\n",
            "Extracting Training Features: 65.71% done\n",
            "Extracting Training Features: 66.13% done\n",
            "Extracting Training Features: 66.56% done\n",
            "Extracting Training Features: 66.99% done\n",
            "Extracting Training Features: 67.41% done\n",
            "Extracting Training Features: 67.84% done\n",
            "Extracting Training Features: 68.27% done\n",
            "Extracting Training Features: 68.69% done\n",
            "Extracting Training Features: 69.12% done\n",
            "Extracting Training Features: 69.55% done\n",
            "Extracting Training Features: 69.97% done\n",
            "Extracting Training Features: 70.40% done\n",
            "Extracting Training Features: 70.83% done\n",
            "Extracting Training Features: 71.25% done\n",
            "Extracting Training Features: 71.68% done\n",
            "Extracting Training Features: 72.11% done\n",
            "Extracting Training Features: 72.53% done\n",
            "Extracting Training Features: 72.96% done\n",
            "Extracting Training Features: 73.39% done\n",
            "Extracting Training Features: 73.81% done\n",
            "Extracting Training Features: 74.24% done\n",
            "Extracting Training Features: 74.67% done\n",
            "Extracting Training Features: 75.09% done\n",
            "Extracting Training Features: 75.52% done\n",
            "Extracting Training Features: 75.95% done\n",
            "Extracting Training Features: 76.37% done\n",
            "Extracting Training Features: 76.80% done\n",
            "Extracting Training Features: 77.23% done\n",
            "Extracting Training Features: 77.65% done\n",
            "Extracting Training Features: 78.08% done\n",
            "Extracting Training Features: 78.51% done\n",
            "Extracting Training Features: 78.93% done\n",
            "Extracting Training Features: 79.36% done\n",
            "Extracting Training Features: 79.79% done\n",
            "Extracting Training Features: 80.21% done\n",
            "Extracting Training Features: 80.64% done\n",
            "Extracting Training Features: 81.07% done\n",
            "Extracting Training Features: 81.49% done\n",
            "Extracting Training Features: 81.92% done\n",
            "Extracting Training Features: 82.35% done\n",
            "Extracting Training Features: 82.77% done\n",
            "Extracting Training Features: 83.20% done\n",
            "Extracting Training Features: 83.63% done\n",
            "Extracting Training Features: 84.05% done\n",
            "Extracting Training Features: 84.48% done\n",
            "Extracting Training Features: 84.91% done\n",
            "Extracting Training Features: 85.33% done\n",
            "Extracting Training Features: 85.76% done\n",
            "Extracting Training Features: 86.19% done\n",
            "Extracting Training Features: 86.61% done\n",
            "Extracting Training Features: 87.04% done\n",
            "Extracting Training Features: 87.47% done\n",
            "Extracting Training Features: 87.89% done\n",
            "Extracting Training Features: 88.32% done\n",
            "Extracting Training Features: 88.75% done\n",
            "Extracting Training Features: 89.17% done\n",
            "Extracting Training Features: 89.60% done\n",
            "Extracting Training Features: 90.03% done\n",
            "Extracting Training Features: 90.45% done\n",
            "Extracting Training Features: 90.88% done\n",
            "Extracting Training Features: 91.31% done\n",
            "Extracting Training Features: 91.73% done\n",
            "Extracting Training Features: 92.16% done\n",
            "Extracting Training Features: 92.59% done\n",
            "Extracting Training Features: 93.01% done\n",
            "Extracting Training Features: 93.44% done\n",
            "Extracting Training Features: 93.87% done\n",
            "Extracting Training Features: 94.29% done\n",
            "Extracting Training Features: 94.72% done\n",
            "Extracting Training Features: 95.15% done\n",
            "Extracting Training Features: 95.57% done\n",
            "Extracting Training Features: 96.00% done\n",
            "Extracting Training Features: 96.43% done\n",
            "Extracting Training Features: 96.85% done\n",
            "Extracting Training Features: 97.28% done\n",
            "Extracting Training Features: 97.71% done\n",
            "Extracting Training Features: 98.13% done\n",
            "Extracting Training Features: 98.56% done\n",
            "Extracting Training Features: 98.99% done\n",
            "Extracting Training Features: 99.41% done\n",
            "Extracting Training Features: 99.84% done\n",
            "Extracting Training Features: 100.00% done\n",
            "Extracting Test Features: 2.56% done\n",
            "Extracting Test Features: 5.12% done\n",
            "Extracting Test Features: 7.68% done\n",
            "Extracting Test Features: 10.24% done\n",
            "Extracting Test Features: 12.80% done\n",
            "Extracting Test Features: 15.36% done\n",
            "Extracting Test Features: 17.92% done\n",
            "Extracting Test Features: 20.48% done\n",
            "Extracting Test Features: 23.04% done\n",
            "Extracting Test Features: 25.60% done\n",
            "Extracting Test Features: 28.16% done\n",
            "Extracting Test Features: 30.72% done\n",
            "Extracting Test Features: 33.28% done\n",
            "Extracting Test Features: 35.84% done\n",
            "Extracting Test Features: 38.40% done\n",
            "Extracting Test Features: 40.96% done\n",
            "Extracting Test Features: 43.52% done\n",
            "Extracting Test Features: 46.08% done\n",
            "Extracting Test Features: 48.64% done\n",
            "Extracting Test Features: 51.20% done\n",
            "Extracting Test Features: 53.76% done\n",
            "Extracting Test Features: 56.32% done\n",
            "Extracting Test Features: 58.88% done\n",
            "Extracting Test Features: 61.44% done\n",
            "Extracting Test Features: 64.00% done\n",
            "Extracting Test Features: 66.56% done\n",
            "Extracting Test Features: 69.12% done\n",
            "Extracting Test Features: 71.68% done\n",
            "Extracting Test Features: 74.24% done\n",
            "Extracting Test Features: 76.80% done\n",
            "Extracting Test Features: 79.36% done\n",
            "Extracting Test Features: 81.92% done\n",
            "Extracting Test Features: 84.48% done\n",
            "Extracting Test Features: 87.04% done\n",
            "Extracting Test Features: 89.60% done\n",
            "Extracting Test Features: 92.16% done\n",
            "Extracting Test Features: 94.72% done\n",
            "Extracting Test Features: 97.28% done\n",
            "Extracting Test Features: 99.84% done\n",
            "Extracting Test Features: 100.00% done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Save Features\n",
        "if DSET_NAME in ('MNIST', 'FMNIST', 'CIFAR10', 'CIFAR100'):\n",
        "    fname_tr = DSET_NAME+'_'+PRETRAINED_MODEL_NAME+'_train.npz'\n",
        "    fname_tst = DSET_NAME+'_'+PRETRAINED_MODEL_NAME+'_test.npz'\n",
        "\n",
        "    np.savez(feature_path + fname_tr, feature_matrices = feature_matrices_tr, classnames = classnames)\n",
        "    np.savez(feature_path + fname_tst, feature_matrices = feature_matrices_tst, classnames = classnames)"
      ],
      "metadata": {
        "id": "5DBOom8q1N1v"
      },
      "execution_count": 54,
      "outputs": []
    }
  ]
}