{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mtrefilek/cs762/blob/main/Private_Classifier_(Opacus).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e56e350",
      "metadata": {
        "id": "4e56e350"
      },
      "outputs": [],
      "source": [
        "MAX_GRAD_NORM = 1.2\n",
        "EPSILON = .5\n",
        "DELTA = 1e-5\n",
        "EPOCHS = 20\n",
        "LR = .1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80696a88",
      "metadata": {
        "id": "80696a88"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 64\n",
        "MAX_PHYSICAL_BATCH_SIZE = 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38558851",
      "metadata": {
        "id": "38558851"
      },
      "outputs": [],
      "source": [
        "#FEATURE_EXTRACTOR_NAME= 'clip-vit-base-patch32'\n",
        "FEATURE_EXTRACTOR_NAME = 'vit-base-patch32-384'\n",
        "#FEATURE_EXTRACTOR_NAME = 'scatternet'\n",
        "DSET_NAME = 'EuroSAT' #('MNIST', 'FMNIST', 'CIFAR10', 'CIFAR100', 'PlantDisease', 'EuroSAT', 'ChestXRay') "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd8342fc",
      "metadata": {
        "id": "fd8342fc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch, os\n",
        "import torchmetrics\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from tqdm.notebook import tqdm\n",
        "from opacus import PrivacyEngine\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from opacus.utils.batch_memory_manager import BatchMemoryManager\n",
        "from opacus.layers.dp_multihead_attention import DPMultiheadAttention\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36742f83",
      "metadata": {
        "id": "36742f83"
      },
      "outputs": [],
      "source": [
        "## Models\n",
        "class LinearNet(nn.Module):\n",
        "    def __init__(self, d_in, d_out):\n",
        "        super(LinearNet, self).__init__()\n",
        "        self.d_in = d_in\n",
        "        self.d_out = d_out\n",
        "        self.linear = nn.Linear(d_in, d_out)\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)\n",
        "    \n",
        "class LinearAdjustedNet(nn.Module):\n",
        "    def __init__(self, d_in, d_out, l, th):\n",
        "        super(LinearAdjustedNet, self).__init__()\n",
        "        self.d_in = d_in\n",
        "        self.d_out = d_out\n",
        "        self.l = l\n",
        "        self.th = th\n",
        "        self.d_ms = [-(d_in//-l) for i in range(l)]\n",
        "        if d_in % l != 0:\n",
        "            self.d_ms[-1] = d_in - self.d_ms[0] * (l-1)\n",
        "        self.linears = nn.ModuleList([nn.Linear(d_m, d_out) for d_m in self.d_ms])\n",
        "    def forward(self, x):\n",
        "        mask = (torch.abs(x) >= self.th).type_as(x)\n",
        "        x *= mask\n",
        "        return torch.stack([f(x[:,i*self.d_ms[i]:(i+1)*self.d_ms[i]]) for i, f in enumerate(self.linears)], dim=-1)\n",
        "    \n",
        "class DPMiniAttentionNet(nn.Module):\n",
        "    def __init__(self, d_in, d_out, l, th, num_heads):\n",
        "        super(DPMiniAttentionNet, self).__init__()\n",
        "        assert d_in % l == 0, \"d_in should be dividable by l.\"\n",
        "        self.d_in = d_in\n",
        "        self.d_out = d_out\n",
        "        self.l = l\n",
        "        self.th = th\n",
        "        self.embed_dim = d_in // l\n",
        "        self.num_heads = num_heads\n",
        "        self.linear = nn.Linear(d_in, d_out)\n",
        "        self.attention = DPMultiheadAttention(embed_dim=self.embed_dim, num_heads=self.num_heads)\n",
        "        \n",
        "        #self.modules = nn.ModuleDict({'linear': nn.Linear(d_in, d_out),\n",
        "        #                             'attention': nn.MultiheadAttention(embed_dim=self.embed_dim, \n",
        "        #                                                                num_heads=self.num_heads, \n",
        "        #                                                                batch_first=True)})\n",
        "    def forward(self, x):\n",
        "#         mask = (torch.abs(x) >= self.th).type_as(x)\n",
        "#         x *= mask\n",
        "        x = torch.stack([x[:,i*self.embed_dim:(i+1)*self.embed_dim] for i in range(self.l)], dim=0)\n",
        "        x = self.attention.forward(x, x, x, need_weights=False)\n",
        "        x = self.linear(torch.cat(x[0].unbind(0),dim=-1).unsqueeze(0))  ## batch_first=False unsqueeze->\n",
        "                                                                        ## squeeze trick is needed \n",
        "                                                                        ## see: https://githubmemory.com/repo/pytorch/opacus/issues/158\n",
        "        #x = self.linear(torch.flatten(x[0], 1, -1)) ## batch_first=True\n",
        "        return x.squeeze(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73e9476e",
      "metadata": {
        "id": "73e9476e"
      },
      "outputs": [],
      "source": [
        "def accuracy(preds, labels):\n",
        "    return (preds == labels).mean()\n",
        "\n",
        "def dptrain(model, train_loader, optimizer, epoch, device):\n",
        "    model.train()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    losses = []\n",
        "    #top1_acc = []\n",
        "    \n",
        "    with BatchMemoryManager(\n",
        "        data_loader=train_loader, \n",
        "        max_physical_batch_size=MAX_PHYSICAL_BATCH_SIZE, \n",
        "        optimizer=optimizer\n",
        "    ) as memory_safe_data_loader:\n",
        "\n",
        "        for i, (images, target) in enumerate(memory_safe_data_loader):   \n",
        "            optimizer.zero_grad()\n",
        "            images = images.to(device)\n",
        "            target = target.to(device)\n",
        "\n",
        "            # compute output\n",
        "            output = model(images)\n",
        "            try:\n",
        "                l = output.shape[2]\n",
        "                #loss = criterion(output, target.expand(l, 1).t())\n",
        "                loss = criterion(output, target.view(target.shape[0],1).expand(target.shape[0], l)) # Slightly more efficient\n",
        "                #score = torch.mean(F.softmax(output, dim=1), dim=-1)\n",
        "            except:\n",
        "                loss = criterion(output, target)\n",
        "                #score = F.softmax(output, dim=1)\n",
        "                \n",
        "            #preds = np.argmax(score.detach().cpu().numpy(), axis=1)\n",
        "            #labels = target.detach().cpu().numpy()\n",
        "\n",
        "            # measure accuracy and record loss\n",
        "            #acc = accuracy(preds, labels)\n",
        "            losses.append(loss.item())\n",
        "            #top1_acc.append(acc)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        epsilon = privacy_engine.get_epsilon(DELTA)\n",
        "        print(f\"\\tTrain Epoch: {epoch} \"\n",
        "              f\"Loss: {np.mean(losses):.6f} \"\n",
        "              #f\"Acc@1: {np.mean(top1_acc) * 100:.6f} \"\n",
        "              f\"(ε = {epsilon:.2f}, δ = {DELTA})\")\n",
        "        \n",
        "def train(model, train_loader, optimizer, epoch, device):\n",
        "    model.train()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    losses = []\n",
        "    #top1_acc = []\n",
        "\n",
        "    for i, (images, target) in enumerate(train_loader):   \n",
        "        optimizer.zero_grad()\n",
        "        images = images.to(device)\n",
        "        target = target.to(device)\n",
        "\n",
        "        # compute output\n",
        "        output = model(images)\n",
        "        try:\n",
        "            l = output.shape[2]\n",
        "            #loss = criterion(output, target.expand(l, 1).t())\n",
        "            loss = criterion(output, target.view(target.shape[0],1).expand(target.shape[0], l)) # Slightly more efficient\n",
        "            #score = torch.mean(F.softmax(output, dim=1), dim=-1)\n",
        "        except:\n",
        "            loss = criterion(output, target)\n",
        "            #score = F.softmax(output, dim=1)\n",
        "\n",
        "        #preds = np.argmax(score.detach().cpu().numpy(), axis=1)\n",
        "        #labels = target.detach().cpu().numpy()\n",
        "\n",
        "        # measure accuracy and record loss\n",
        "        #acc = accuracy(preds, labels)\n",
        "        losses.append(loss.item())\n",
        "        #top1_acc.append(acc)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"\\tTrain Epoch: {epoch} \"\n",
        "          f\"Loss: {np.mean(losses):.6f} \"\n",
        "          #f\"Acc@1: {np.mean(top1_acc) * 100:.6f} \"\n",
        "         )\n",
        "        \n",
        "def test(model, test_loader, device):\n",
        "    model.eval()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    losses = []\n",
        "    top1_acc = []\n",
        "    auroc = torchmetrics.AUROC()\n",
        "    auroc.num_classes = N_CLS\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, target in test_loader:\n",
        "            images = images.to(device)\n",
        "            target = target.to(device)\n",
        "\n",
        "            output = model(images)\n",
        "            try:\n",
        "                l = output.shape[2]\n",
        "                #loss = criterion(output, target.repeat(l, 1).t())\n",
        "                loss = criterion(output, target.view(target.shape[0],1).expand(target.shape[0], l))\n",
        "                score = torch.mean(F.softmax(output, dim=1), dim=-1)\n",
        "            except:\n",
        "                loss = criterion(output, target)\n",
        "                score = F.softmax(output, dim=1)\n",
        "\n",
        "            auroc.update(score, target)\n",
        "            preds = np.argmax(score.detach().cpu().numpy(), axis=1)\n",
        "            labels = target.detach().cpu().numpy()\n",
        "            acc = accuracy(preds, labels)\n",
        "\n",
        "            losses.append(loss.item())\n",
        "            top1_acc.append(acc)\n",
        "\n",
        "    top1_avg = np.mean(top1_acc)\n",
        "    auc = auroc.compute()\n",
        "\n",
        "    print(f\"\\tTest set:\"\n",
        "          f\"Loss: {np.mean(losses):.6f} \"\n",
        "          f\"Acc: {top1_avg * 100:.6f} \"\n",
        "          f\"AUC: {auc:.6f} \")\n",
        "    return np.mean(top1_acc), auc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2b8413f",
      "metadata": {
        "id": "e2b8413f"
      },
      "outputs": [],
      "source": [
        "dset_path = os.getcwd().replace('\\\\','/')+'/extracted_features/'+DSET_NAME+'_'+FEATURE_EXTRACTOR_NAME"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d557c1d",
      "metadata": {
        "id": "9d557c1d"
      },
      "outputs": [],
      "source": [
        "if DSET_NAME in ('PlantDisease', 'EuroSAT'):\n",
        "    feature_set = np.load(dset_path+'.npz', allow_pickle=True)\n",
        "    X_tr, X_tst = [], []\n",
        "    for f in feature_set['feature_matrices']:\n",
        "        n_tst = len(f) // 10\n",
        "        f = np.random.permutation(f)\n",
        "        X_tr.append(f[n_tst:])\n",
        "        X_tst.append(f[:n_tst])\n",
        "else:\n",
        "    feature_set = np.load(dset_path+'_train.npz', allow_pickle=True)\n",
        "    feature_set_tst = np.load(dset_path+'_test.npz', allow_pickle=True)\n",
        "    X_tr = list(feature_set['feature_matrices'])\n",
        "    X_tst = list(feature_set_tst['feature_matrices'])\n",
        "\n",
        "class_sizes_tr = [len(f) for f in X_tr]\n",
        "class_sizes_tst = [len(f) for f in X_tst]\n",
        "n_tr = np.sum(class_sizes_tr)\n",
        "n_tst = np.sum(class_sizes_tst)\n",
        "N_CLS = len(X_tr)\n",
        "D_DIM = len(X_tr[0][0])\n",
        "X_tr, X_tst = (np.vstack(X_tr), np.vstack(X_tst))\n",
        "y_tr, y_tst = (np.zeros(n_tr), np.zeros(n_tst))\n",
        "l = 0\n",
        "for i, k in enumerate(class_sizes_tr):\n",
        "    y_tr[l:l+k].fill(i)\n",
        "    l += k\n",
        "l = 0\n",
        "for i, k in enumerate(class_sizes_tst):\n",
        "    y_tst[l:l+k].fill(i)\n",
        "    l += k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27f5a50b",
      "metadata": {
        "id": "27f5a50b"
      },
      "outputs": [],
      "source": [
        "trainset = TensorDataset(torch.tensor(X_tr).float(), torch.tensor(y_tr).long())\n",
        "train_loader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a4e8e57",
      "metadata": {
        "id": "2a4e8e57"
      },
      "outputs": [],
      "source": [
        "testset = TensorDataset(torch.tensor(X_tst).float(), torch.tensor(y_tst).long())\n",
        "test_loader = DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ea5a0bc",
      "metadata": {
        "id": "5ea5a0bc"
      },
      "outputs": [],
      "source": [
        "## Specify the model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#device = torch.device(\"cpu\")\n",
        "model = LinearNet(D_DIM, N_CLS)\n",
        "#model = LinearAdjustedNet(D_DIM, N_CLS, 16, .1)\n",
        "#model = DPMiniAttentionNet(D_DIM, N_CLS, 16, .1, 4)\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a155f5ff",
      "metadata": {
        "id": "a155f5ff"
      },
      "outputs": [],
      "source": [
        "## Loss & Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=LR)\n",
        "#optimizer = optim.RMSprop(model.parameters(), lr=LR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4469ab9",
      "metadata": {
        "id": "d4469ab9"
      },
      "outputs": [],
      "source": [
        "if model.__class__.__name__ == 'DPMiniAttentionNet':\n",
        "    batch_first = False\n",
        "else:\n",
        "    batch_first = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c827089f",
      "metadata": {
        "id": "c827089f"
      },
      "outputs": [],
      "source": [
        "## Model Check\n",
        "# from opacus.validators import ModuleValidator\n",
        "# errors = ModuleValidator.validate(model, strict=False)\n",
        "# errors[-5:]\n",
        "# model = ModuleValidator.fix(model)\n",
        "# ModuleValidator.validate(model, strict=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7fa1fc1",
      "metadata": {
        "scrolled": true,
        "id": "e7fa1fc1",
        "outputId": "416f8cc3-e3c0-4178-c9fc-e05657aa3eb0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using sigma=1.9133447265624999 and C=1.2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\user\\anaconda3\\lib\\site-packages\\opacus\\privacy_engine.py:100: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
            "  warnings.warn(\n",
            "C:\\Users\\user\\anaconda3\\lib\\site-packages\\opacus\\accountants\\analysis\\rdp.py:320: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Privacy Engine\n",
        "privacy_engine = PrivacyEngine()\n",
        "\n",
        "model, optimizer, data_loader = privacy_engine.make_private_with_epsilon(\n",
        "    module=model,\n",
        "    optimizer=optimizer,\n",
        "    data_loader=train_loader,\n",
        "    epochs=EPOCHS,\n",
        "    target_epsilon=EPSILON,\n",
        "    target_delta=DELTA,\n",
        "    max_grad_norm=MAX_GRAD_NORM,\n",
        "    batch_first=batch_first,\n",
        ")\n",
        "\n",
        "print(f\"Using sigma={optimizer.noise_multiplier} and C={MAX_GRAD_NORM}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2371482",
      "metadata": {
        "id": "d2371482",
        "outputId": "5be3c8b1-88e8-49f2-a982-ac4d2907084d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'GradSampleModule'"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.__class__.__name__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec7b507b",
      "metadata": {
        "scrolled": false,
        "colab": {
          "referenced_widgets": [
            "2ea9d24a16f3479d83555134c822d90f"
          ]
        },
        "id": "ec7b507b",
        "outputId": "0057d4aa-8935-4c9e-f0a4-89215ddc308e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2ea9d24a16f3479d83555134c822d90f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch:   0%|          | 0/20 [00:00<?, ?epoch/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\user\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1025: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Epoch: 1 Loss: 0.961169 (ε = 0.18, δ = 1e-05)\n",
            "\tTrain Epoch: 2 Loss: 0.511725 (ε = 0.20, δ = 1e-05)\n",
            "\tTrain Epoch: 3 Loss: 0.434339 (ε = 0.22, δ = 1e-05)\n",
            "\tTrain Epoch: 4 Loss: 0.398961 (ε = 0.24, δ = 1e-05)\n",
            "\tTrain Epoch: 5 Loss: 0.383174 (ε = 0.26, δ = 1e-05)\n",
            "\tTrain Epoch: 6 Loss: 0.370495 (ε = 0.27, δ = 1e-05)\n",
            "\tTrain Epoch: 7 Loss: 0.369004 (ε = 0.29, δ = 1e-05)\n",
            "\tTrain Epoch: 8 Loss: 0.367678 (ε = 0.31, δ = 1e-05)\n",
            "\tTrain Epoch: 9 Loss: 0.362459 (ε = 0.33, δ = 1e-05)\n",
            "\tTrain Epoch: 10 Loss: 0.357179 (ε = 0.35, δ = 1e-05)\n",
            "\tTrain Epoch: 11 Loss: 0.357180 (ε = 0.36, δ = 1e-05)\n",
            "\tTrain Epoch: 12 Loss: 0.359430 (ε = 0.38, δ = 1e-05)\n",
            "\tTrain Epoch: 13 Loss: 0.356147 (ε = 0.40, δ = 1e-05)\n",
            "\tTrain Epoch: 14 Loss: 0.354013 (ε = 0.41, δ = 1e-05)\n",
            "\tTrain Epoch: 15 Loss: 0.354690 (ε = 0.43, δ = 1e-05)\n",
            "\tTrain Epoch: 16 Loss: 0.355120 (ε = 0.44, δ = 1e-05)\n",
            "\tTrain Epoch: 17 Loss: 0.356341 (ε = 0.46, δ = 1e-05)\n",
            "\tTrain Epoch: 18 Loss: 0.357066 (ε = 0.47, δ = 1e-05)\n",
            "\tTrain Epoch: 19 Loss: 0.361582 (ε = 0.49, δ = 1e-05)\n",
            "\tTrain Epoch: 20 Loss: 0.360828 (ε = 0.50, δ = 1e-05)\n"
          ]
        }
      ],
      "source": [
        "for epoch in tqdm(range(EPOCHS), desc=\"Epoch\", unit=\"epoch\"):\n",
        "    dptrain(model, train_loader, optimizer, epoch + 1, device)\n",
        "    #train(model, train_loader, optimizer, epoch + 1, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c62a0444",
      "metadata": {
        "id": "c62a0444",
        "outputId": "8efdac9f-1b56-4c3f-d15a-4cf5ff2d1e84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTest set:Loss: 0.383763 Acc: 89.886143 AUC: 0.991445 \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\user\\anaconda3\\lib\\site-packages\\torchmetrics\\utilities\\prints.py:36: UserWarning: Metric `AUROC` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n",
            "  warnings.warn(*args, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "acc, auc = test(model, test_loader, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c92d492b",
      "metadata": {
        "id": "c92d492b",
        "outputId": "596052a1-71d1-416f-ba7c-1a64be736baa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.49978663601343887"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "privacy_engine.get_epsilon(DELTA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6bbfd78",
      "metadata": {
        "id": "f6bbfd78"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "927e3b70",
      "metadata": {
        "id": "927e3b70"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "Private Classifier (Opacus).ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}