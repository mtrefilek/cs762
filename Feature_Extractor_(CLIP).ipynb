{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mtrefilek/cs762/blob/main/Feature_Extractor_(CLIP).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SFGEo3WOhLdK"
      },
      "outputs": [],
      "source": [
        "from transformers import CLIPTokenizer, CLIPProcessor, CLIPModel\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RXpz2t7thLdO"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "cwd = os.getcwd().replace('\\\\','/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVcuOn1ohLdR",
        "outputId": "486b2640-a85d-478e-89ec-edffcea1f4f1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ftfy or spacy is not installed using BERT BasicTokenizer instead of ftfy.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "CLIPModel(\n",
              "  (text_model): CLIPTextTransformer(\n",
              "    (embeddings): CLIPTextEmbeddings(\n",
              "      (token_embedding): Embedding(49408, 512)\n",
              "      (position_embedding): Embedding(77, 512)\n",
              "    )\n",
              "    (encoder): CLIPEncoder(\n",
              "      (layers): ModuleList(\n",
              "        (0): CLIPEncoderLayer(\n",
              "          (self_attn): CLIPAttention(\n",
              "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): CLIPMLP(\n",
              "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          )\n",
              "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (1): CLIPEncoderLayer(\n",
              "          (self_attn): CLIPAttention(\n",
              "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): CLIPMLP(\n",
              "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          )\n",
              "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (2): CLIPEncoderLayer(\n",
              "          (self_attn): CLIPAttention(\n",
              "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): CLIPMLP(\n",
              "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          )\n",
              "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (3): CLIPEncoderLayer(\n",
              "          (self_attn): CLIPAttention(\n",
              "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): CLIPMLP(\n",
              "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          )\n",
              "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (4): CLIPEncoderLayer(\n",
              "          (self_attn): CLIPAttention(\n",
              "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): CLIPMLP(\n",
              "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          )\n",
              "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (5): CLIPEncoderLayer(\n",
              "          (self_attn): CLIPAttention(\n",
              "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): CLIPMLP(\n",
              "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          )\n",
              "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (6): CLIPEncoderLayer(\n",
              "          (self_attn): CLIPAttention(\n",
              "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): CLIPMLP(\n",
              "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          )\n",
              "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (7): CLIPEncoderLayer(\n",
              "          (self_attn): CLIPAttention(\n",
              "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): CLIPMLP(\n",
              "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          )\n",
              "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (8): CLIPEncoderLayer(\n",
              "          (self_attn): CLIPAttention(\n",
              "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): CLIPMLP(\n",
              "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          )\n",
              "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (9): CLIPEncoderLayer(\n",
              "          (self_attn): CLIPAttention(\n",
              "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): CLIPMLP(\n",
              "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          )\n",
              "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (10): CLIPEncoderLayer(\n",
              "          (self_attn): CLIPAttention(\n",
              "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): CLIPMLP(\n",
              "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          )\n",
              "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (11): CLIPEncoderLayer(\n",
              "          (self_attn): CLIPAttention(\n",
              "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): CLIPMLP(\n",
              "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          )\n",
              "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (vision_model): CLIPVisionTransformer(\n",
              "    (embeddings): CLIPVisionEmbeddings(\n",
              "      (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
              "      (position_embedding): Embedding(50, 768)\n",
              "    )\n",
              "    (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    (encoder): CLIPEncoder(\n",
              "      (layers): ModuleList(\n",
              "        (0): CLIPEncoderLayer(\n",
              "          (self_attn): CLIPAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): CLIPMLP(\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (1): CLIPEncoderLayer(\n",
              "          (self_attn): CLIPAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): CLIPMLP(\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (2): CLIPEncoderLayer(\n",
              "          (self_attn): CLIPAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): CLIPMLP(\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (3): CLIPEncoderLayer(\n",
              "          (self_attn): CLIPAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): CLIPMLP(\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (4): CLIPEncoderLayer(\n",
              "          (self_attn): CLIPAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): CLIPMLP(\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (5): CLIPEncoderLayer(\n",
              "          (self_attn): CLIPAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): CLIPMLP(\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (6): CLIPEncoderLayer(\n",
              "          (self_attn): CLIPAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): CLIPMLP(\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (7): CLIPEncoderLayer(\n",
              "          (self_attn): CLIPAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): CLIPMLP(\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (8): CLIPEncoderLayer(\n",
              "          (self_attn): CLIPAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): CLIPMLP(\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (9): CLIPEncoderLayer(\n",
              "          (self_attn): CLIPAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): CLIPMLP(\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (10): CLIPEncoderLayer(\n",
              "          (self_attn): CLIPAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): CLIPMLP(\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (11): CLIPEncoderLayer(\n",
              "          (self_attn): CLIPAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): CLIPMLP(\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (visual_projection): Linear(in_features=768, out_features=512, bias=False)\n",
              "  (text_projection): Linear(in_features=512, out_features=512, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#%%capture\n",
        "PRETRAINED_MODEL_NAME = 'clip-vit-base-patch32'\n",
        "\n",
        "### Model Preparation\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model_path = cwd+'/pretrained_models/'+PRETRAINED_MODEL_NAME\n",
        "\n",
        "# model = CLIPModel.from_pretrained('openai/'+PRETRAINED_MODEL_NAME)\n",
        "# processor = CLIPProcessor.from_pretrained('openai/'+PRETRAINED_MODEL_NAME)\n",
        "# model.save_pretrained(model_path)\n",
        "# processor.save_pretrained(model_path)\n",
        "\n",
        "model = CLIPModel.from_pretrained(model_path)\n",
        "processor = CLIPProcessor.from_pretrained(model_path)\n",
        "model.eval()\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YSCn82nDAinG"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "from copy import deepcopy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQnnWAfsR9yh"
      },
      "outputs": [],
      "source": [
        "DSET_NAME = 'EuroSAT' #('MNIST', 'FMNIST', 'CIFAR10', 'CIFAR100', 'PlantDisease', 'EuroSAT', 'ChestXRay') 'ISIC2018', 'TinyImageNet' are not yet implemented"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MLcpNqOEhXlX",
        "outputId": "6e692d84-377e-48ad-a24d-8ca8ccf29a06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Annual Crop\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\user\\anaconda3\\lib\\site-packages\\transformers\\feature_extraction_utils.py:158: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_new.cpp:201.)\n",
            "  tensor = as_tensor(value)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Annual Crop\n",
            "Forest\n",
            "Forest\n",
            "Herbaceous Vegetation\n",
            "Herbaceous Vegetation\n",
            "Highway\n",
            "Highway\n",
            "Industrial\n",
            "Industrial\n",
            "Pasture\n",
            "Pasture\n",
            "Permanent Crop\n",
            "Permanent Crop\n",
            "Residential\n",
            "Residential\n",
            "River\n",
            "River\n",
            "Sea Lake\n",
            "Sea Lake\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\user\\anaconda3\\lib\\site-packages\\numpy\\core\\_asarray.py:171: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  return array(a, dtype, copy=False, order=order, subok=True)\n"
          ]
        }
      ],
      "source": [
        "### Dataset Preparation\n",
        "from torchvision import datasets\n",
        "from glob import glob, iglob\n",
        "from PIL import Image\n",
        "\n",
        "def batch(iterable, n=1):\n",
        "    l = len(iterable)\n",
        "    for ndx in range(0, l, n):\n",
        "        yield iterable[ndx:min(ndx + n, l)]\n",
        "\n",
        "#import torchvision\n",
        "#transform=torchvision.transforms.Compose([\n",
        "    # you can add other transformations in this list\n",
        "    #torchvision.transforms.ToTensor()\n",
        "#])\n",
        "dset_path = cwd + '/dataset'\n",
        "feature_path = cwd+'/extracted_features/'\n",
        "\n",
        "if DSET_NAME=='MNIST': # MNIST\n",
        "    mnist_train = datasets.MNIST(root=dset_path, train=True, download=True)#, transform=transform)\n",
        "    mnist_test = datasets.MNIST(root=dset_path, train=False, download=True)#, transform=transform)\n",
        "    imgs_tr, imgs_tst, labels_tr, labels_tst = [], [], [], []\n",
        "    for (img, label) in mnist_train:\n",
        "        imgs_tr.append(img.convert(mode='RGB'))\n",
        "        labels_tr.append(label)\n",
        "    for (img, label) in mnist_test:\n",
        "        imgs_tst.append(img.convert(mode='RGB'))\n",
        "        labels_tst.append(label)\n",
        "    classnames = [str(a) for a in range(10)]\n",
        "    t = ['zero', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine']\n",
        "    classtexts = ['This is number '+a+', '+t[int(a)]+'.' for a in classnames]\n",
        "\n",
        "elif DSET_NAME=='FMNIST': # Fashion-MNIST\n",
        "    fmnist_train = datasets.FashionMNIST(root=dset_path, train=True, download=True)#, transform=transform)\n",
        "    fmnist_test = datasets.FashionMNIST(root=dset_path, train=False, download=True)#, transform=transform)\n",
        "    imgs_tr, imgs_tst, labels_tr, labels_tst = [], [], [], []\n",
        "    for (img, label) in fmnist_train:\n",
        "        imgs_tr.append(img.convert(mode='RGB'))\n",
        "        labels_tr.append(label)\n",
        "    for (img, label) in fmnist_test:\n",
        "        imgs_tst.append(img.convert(mode='RGB'))\n",
        "        labels_tst.append(label)\n",
        "    classnames = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "    classtexts = []\n",
        "    for a in classnames:\n",
        "        if a[0] in 'aeiouAEIOU':\n",
        "            classtexts.append('This is a picture of an '+a+'.')\n",
        "        else:\n",
        "            classtexts.append('This is a picture of a '+a+'.')\n",
        "\n",
        "elif DSET_NAME=='CIFAR10': # CIFAR-10\n",
        "    cifar10_train = datasets.CIFAR10(root=dset_path, train=True, download=True)#, transform=transform)\n",
        "    cifar10_test = datasets.CIFAR10(root=dset_path, train=False, download=True)#, transform=transform)\n",
        "    imgs_tr, imgs_tst, labels_tr, labels_tst = [], [], [], []\n",
        "    for (img, label) in cifar10_train:\n",
        "        imgs_tr.append(img)\n",
        "        labels_tr.append(label)\n",
        "    for (img, label) in cifar10_test:\n",
        "        imgs_tst.append(img)\n",
        "        labels_tst.append(label)\n",
        "    meta = pickle.load( open(dset_path+'/cifar-10-batches-py/batches.meta','rb') )\n",
        "    classnames = meta['label_names']\n",
        "    classtexts = []\n",
        "    for a in classnames:\n",
        "        if a[0] in 'aeiouAEIOU':\n",
        "            classtexts.append('This is a picture of an '+a+'.')\n",
        "        else:\n",
        "            classtexts.append('This is a picture of a '+a+'.')\n",
        "\n",
        "elif DSET_NAME=='CIFAR100': # CIFAR-100\n",
        "    cifar100_train = datasets.CIFAR100(root=dset_path, train=True, download=True)#, transform=transform)\n",
        "    cifar100_test = datasets.CIFAR100(root=dset_path, train=False, download=True)#, transform=transform)\n",
        "    imgs_tr, imgs_tst, labels_tr, labels_tst = [], [], [], []\n",
        "    for (img, label) in cifar100_train:\n",
        "        imgs_tr.append(img)\n",
        "        labels_tr.append(label)\n",
        "    for (img, label) in cifar100_test:\n",
        "        imgs_tst.append(img)\n",
        "        labels_tst.append(label)\n",
        "    meta = pickle.load( open(dset_path+'/cifar-100-python/meta','rb') )\n",
        "    classnames = meta['fine_label_names']\n",
        "    classtexts = []\n",
        "    for a in classnames:\n",
        "        if a[0] in 'aeiouAEIOU':\n",
        "            classtexts.append('This is a picture of an '+a+'.')\n",
        "        else:\n",
        "            classtexts.append('This is a picture of a '+a+'.')\n",
        "\n",
        "elif DSET_NAME in ('PlantDisease','EuroSAT'): # PlantDisease or EuroSAT\n",
        "    list_dirs = glob(dset_path+'/'+DSET_NAME+'/*/')\n",
        "    list_dirs = [l.replace('\\\\','/') for l in list_dirs]\n",
        "    classnames = [d.split('/')[-2] for d in list_dirs]\n",
        "    classtexts = []\n",
        "    if DSET_NAME == 'PlantDisease':\n",
        "        for c in classnames:\n",
        "            a = c.split('___')[0].replace('_', ' ')\n",
        "            b = c.split('___')[1].replace('_', ' ')\n",
        "            if b == 'healthy':\n",
        "                ab = b + ' ' + a\n",
        "            else:\n",
        "                ab = a + ' with ' + b\n",
        "            if ab[0] in 'aeiouAEIOU':\n",
        "                ab = 'an ' + ab\n",
        "            else:\n",
        "                ab = 'a ' + ab\n",
        "            classtexts.append('This is a picture of '+ab+'.')\n",
        "    elif DSET_NAME == 'EuroSAT':\n",
        "        for a in classnames:\n",
        "            if a in ('Industrial', 'Residential'):\n",
        "                a += ' area'\n",
        "            if a[0] in 'aeiouAEIOU':\n",
        "                classtexts.append('This is a satellite picture of an '+a+'.')\n",
        "            else:\n",
        "                classtexts.append('This is a satellite picture of a '+a+'.')\n",
        "    feature_matrices = []\n",
        "    labels = []\n",
        "    for i, classname in enumerate(classnames):\n",
        "        imgs = []\n",
        "        for f in iglob(dset_path+'/'+DSET_NAME+'/'+classname+'/*'):\n",
        "            tmp = Image.open(f)\n",
        "            img = tmp.copy().convert(mode='RGB')\n",
        "            imgs.append(img)\n",
        "        print(classname)\n",
        "        feature_matrix = np.zeros((1, 512))\n",
        "        for img_batch in batch(imgs, n=128):\n",
        "            inputs = processor(images=img_batch, return_tensors=\"pt\")\n",
        "            img_features = model.get_image_features(inputs['pixel_values'].to(device)).cpu().detach().numpy()\n",
        "            feature_matrix = np.concatenate((feature_matrix, img_features), axis=0)\n",
        "        feature_matrices.append(deepcopy(feature_matrix[1:]))\n",
        "        print(classname)\n",
        "    inputs = processor(text=classtexts, return_tensors=\"pt\", padding=True).to(device)\n",
        "    classtext_embeddings = model.get_text_features(**inputs).cpu().detach().numpy()\n",
        "    fname = DSET_NAME+'_'+PRETRAINED_MODEL_NAME+'.npz'\n",
        "    np.savez(feature_path + fname, feature_matrices = feature_matrices, classnames = classnames, classtexts=classtexts, classtext_embeddings = classtext_embeddings)\n",
        "\n",
        "elif DSET_NAME=='ChestXRay': # Chest X-Ray\n",
        "    for split in ('train', 'test'):\n",
        "        list_dirs = glob(dset_path+'/'+DSET_NAME+'/'+split+'/*/')\n",
        "        list_dirs = [l.replace('\\\\','/') for l in list_dirs]\n",
        "        classnames = [d.split('/')[-2] for d in list_dirs]\n",
        "        classtexts = []\n",
        "        for a in classnames:\n",
        "            if a=='NORMAL':\n",
        "                classtexts.append('This is a chest x-ray image of a '+a.lower()+' (healty) person.')\n",
        "            elif a=='PNEUMONIA':\n",
        "                classtexts.append('This is a chest x-ray image of a patient with '+a.lower()+'.')\n",
        "        feature_matrices = []\n",
        "        labels = []\n",
        "        for i, classname in enumerate(classnames):\n",
        "            feature_matrix = np.zeros((1, 512))\n",
        "            for f in iglob(dset_path+'/'+DSET_NAME+'/'+split+'/'+classname+'/*'):\n",
        "                tmp = Image.open(f)\n",
        "                img = tmp.copy().convert(mode='RGB')\n",
        "                inputs = processor(images=img, return_tensors=\"pt\")\n",
        "                img_features = model.get_image_features(inputs['pixel_values'].to(device)).cpu().detach().numpy()\n",
        "                feature_matrix = np.concatenate((feature_matrix, img_features), axis=0)\n",
        "            feature_matrices.append(deepcopy(feature_matrix[1:]))\n",
        "            print(classname)\n",
        "        inputs = processor(text=classtexts, return_tensors=\"pt\", padding=True).to(device)\n",
        "        classtext_embeddings = model.get_text_features(**inputs).cpu().detach().numpy()\n",
        "        fname = DSET_NAME+'_'+PRETRAINED_MODEL_NAME+'_'+split+'.npz'\n",
        "        np.savez(feature_path + fname, feature_matrices = feature_matrices, classnames = classnames, classtexts=classtexts, classtext_embeddings = classtext_embeddings)\n",
        "\n",
        "### Extract Features\n",
        "if DSET_NAME in ('MNIST', 'FMNIST', 'CIFAR10', 'CIFAR100'):\n",
        "    n_tr = len(imgs_tr)\n",
        "    feature_matrix_tr = np.zeros((1, 512))\n",
        "    for img_batch in batch(imgs_tr, n=128):\n",
        "        inputs = processor(images=img_batch, return_tensors=\"pt\")\n",
        "        img_features_tr = model.get_image_features(inputs['pixel_values'].to(device)).cpu().detach().numpy()\n",
        "        feature_matrix_tr = np.concatenate((feature_matrix_tr, img_features_tr), axis=0)\n",
        "        print('Extracting Training Features: {0:.2f}% done'.format(100*len(feature_matrix_tr[1:])/n_tr) )\n",
        "    feature_matrix_tr = feature_matrix_tr[1:]\n",
        "\n",
        "    n_cls = np.max(labels_tr)+1\n",
        "    labels_tr = np.array(labels_tr)\n",
        "    feature_matrices_tr = []\n",
        "    for i in range(n_cls):\n",
        "        feature_matrices_tr.append(feature_matrix_tr[labels_tr==i])\n",
        "\n",
        "    n_tst = len(imgs_tst)\n",
        "    feature_matrix_tst = np.zeros((1, 512))\n",
        "    for img_batch in batch(imgs_tst, n=128):\n",
        "        inputs = processor(images=img_batch, return_tensors=\"pt\")\n",
        "        img_features_tst = model.get_image_features(inputs['pixel_values'].to(device)).cpu().detach().numpy()\n",
        "        feature_matrix_tst = np.concatenate((feature_matrix_tst, img_features_tst), axis=0)\n",
        "        print('Extracting Test Features: {0:.2f}% done'.format(100*len(feature_matrix_tst[1:])/n_tst) )\n",
        "    feature_matrix_tst = feature_matrix_tst[1:]\n",
        "\n",
        "    n_cls = np.max(labels_tst)+1\n",
        "    labels_tst = np.array(labels_tst)\n",
        "    feature_matrices_tst = []\n",
        "    for i in range(n_cls):\n",
        "        feature_matrices_tst.append(feature_matrix_tst[labels_tst==i])\n",
        "        \n",
        "    inputs = processor(text=classtexts, return_tensors=\"pt\", padding=True).to(device)\n",
        "    classtext_embeddings = model.get_text_features(**inputs).cpu().detach().numpy()\n",
        "    \n",
        "### Save Features\n",
        "if DSET_NAME in ('MNIST', 'FMNIST', 'CIFAR10', 'CIFAR100'):\n",
        "    fname_tr = DSET_NAME+'_'+PRETRAINED_MODEL_NAME+'_train.npz'\n",
        "    fname_tst = DSET_NAME+'_'+PRETRAINED_MODEL_NAME+'_test.npz'\n",
        "\n",
        "    np.savez(feature_path + fname_tr, feature_matrices = feature_matrices_tr, classnames = classnames, classtexts = classtexts, classtext_embeddings = classtext_embeddings)\n",
        "    np.savez(feature_path + fname_tst, feature_matrices = feature_matrices_tst, classnames = classnames, classtexts = classtexts, classtext_embeddings = classtext_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hR5XeOTwhLdc"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Feature Extractor (CLIP).ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}